{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg4RcFZZ-miJ"
   },
   "source": [
    "# MNIST Classification with Pytorch\n",
    "\n",
    "In this notebook we demonstrate how to build a classification pipeline for the MNIST problem using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9t3XZaU-miL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z01jjm8O-miM"
   },
   "source": [
    "### Loading data\n",
    "\n",
    "As we did in our previous lessons, we will load the images, normalize them and cast the class IDs to one-hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "comq4uL1-miM",
    "outputId": "9c13af81-9fdd-4a7d-ac40-aaf1201b36bd"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Dataset params\n",
    "num_classes = 10\n",
    "size = x_train.shape[1]\n",
    "\n",
    "# Normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Train set:   ', len(y_train), 'samples')\n",
    "print('Test set:    ', len(y_test), 'samples')\n",
    "print('Sample dims: ', x_train.shape)\n",
    "\n",
    "num_samples = len(y_train)\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJJJVQf2-miN"
   },
   "source": [
    "### Build Classifier\n",
    "\n",
    "Now we are going to build a classifier model using pytorch. For this purpose, we will inherit from the nn.Module object. Note that we ned to indicate pytorch how the forward pass look like (so it can also automatically comp√πte the backward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjsk2cx5elqj"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=---)\n",
    "net = Dense(10, 'relu')(inputs)\n",
    "branch1 = Dense(2)(net)\n",
    "branch2 = Dense(2)(net)\n",
    "out = add([branch1, branch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ugda2wC-miN"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.out = nn.Linear(in_features=32*7*7, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # output shape (14, 14, 16)\n",
    "        x = self.conv2(x) # output shape (7, 7, 32)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uezsr8A7-miN"
   },
   "source": [
    "### Prepare for Training\n",
    "\n",
    "At this stage, we need to build the model, define the loss and the optimizer to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuFQd-Qk-miO"
   },
   "outputs": [],
   "source": [
    "model = Classifier().to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZXoFQWv-miO",
    "outputId": "d82529e0-b3a5-4f1f-aa82-027612ae1cee"
   },
   "outputs": [],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KNOpkfo-miO"
   },
   "source": [
    "Unlike keras, pytorch cannot automatically work with numpy arrays. We need to parse the arrays to pytorch tensors.\n",
    "\n",
    "**Important:** By default, pytorch uses channels-first ordering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0EjaPRfmsF5",
    "outputId": "229755f9-c2dd-4cab-87ec-1c95925df81d"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88IiLb_g-miP"
   },
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(x_test[:, np.newaxis, ...]).float().cuda()\n",
    "y_test = torch.from_numpy(y_test).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7DSX0t2m3V0",
    "outputId": "cc6012f8-c3ea-45ab-dbe8-29fe47d740c3"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLSX_J4z-miP"
   },
   "source": [
    "### Built the Pipeline\n",
    "\n",
    "Finally, we have to define the training loops and the evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6jOrCA6-miP",
    "outputId": "32170683-cbc9-400b-bfc2-b33b7089add7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # You might want to set some metrics to track\n",
    "    loss_monitor, acc_monitor = [], []\n",
    "\n",
    "    # Training loop, i.e., what do we do with each batch of data?\n",
    "    for idx in range(0, num_samples//batch_size):\n",
    "        # Select samples for current batch\n",
    "        x = x_train[idx*batch_size:(idx+1)*batch_size, np.newaxis, ...]\n",
    "        y = y_train[idx*batch_size:(idx+1)*batch_size, ...]\n",
    "        # Convert batch to pytorch tensors\n",
    "        x = torch.from_numpy(x).float().cuda()\n",
    "        y = torch.from_numpy(y).float().cuda()\n",
    "\n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (runs batch data through the model)\n",
    "        outputs = model(x)\n",
    "\n",
    "        # Compute loss function\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Run backward pass, i.e., compute gradients of loss function with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply optimizer, i.e., update network weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update monitors (if you have any)\n",
    "        loss_monitor.append(loss.item())\n",
    "        acc_monitor.append(torch.sum(torch.argmax(outputs, axis=1) == torch.argmax(y, axis=1)).item()/batch_size)\n",
    "\n",
    "    # Logs for tracking the progress\n",
    "    print('Epoch ' + str(epoch) + ' | \\t loss: ' + str(np.mean(loss_monitor)), '\\t' + str(np.mean(acc_monitor)))\n",
    "\n",
    "    # Validation step\n",
    "    # Set model to evaluation mode (avoids computing gradients and switches off any parameter tracings)\n",
    "    model.eval()\n",
    "    # Get predictions from test data\n",
    "    outputs = model(x_test)\n",
    "    # Log performance\n",
    "    print('Eval  ' + str(torch.sum(torch.argmax(outputs, axis=1) == torch.argmax(y_test, axis=1)).item()/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vZEdHLC-miP",
    "outputId": "b3727c1d-7861-4eb8-b738-4a25c96da088"
   },
   "outputs": [],
   "source": [
    "print('Eval  \\t\\t' + str(torch.sum(torch.argmax(outputs, axis=1) == torch.argmax(y_test, axis=1)).item()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5P9aeLaqlK9",
    "outputId": "99782fbb-60a7-4aa5-a863-e7840d96eeb9"
   },
   "outputs": [],
   "source": [
    "model.conv1[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
