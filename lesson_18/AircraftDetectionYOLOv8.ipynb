{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b57487",
   "metadata": {},
   "source": [
    "# Aircraft Detection with YOLOv8\n",
    "\n",
    "In this notebook we will demonstrate how to use and fine-tune the YOLOv8 model to detect aircrafts on the ground.\n",
    "\n",
    "The [YOLOv8](https://github.com/ultralytics/ultralytics) architecture is developed by Ultralytics and you can easilly install all the required tools by runnnig:\n",
    "```console\n",
    "pip install ultralytics\n",
    "```\n",
    "\n",
    "Now let's check that everything has been installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839fc3b",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We wil lbe working with the [Airbus](https://www.kaggle.com/datasets/airbusgeo/airbus-aircrafts-sample-dataset) satellite dataset developed by Aribus Defense and Space Intelligence to detect grounded aircrafts.\n",
    "\n",
    "The dataset contains 103 extract of satellite imagery at roughly 50 cm resolution. Each each image is stored as a JPEG file of size 2560 x 2560 pixels (i.e. 1280 meters on ground). The locations are various airports worldwide. \n",
    "\n",
    "All aircrafts have been annotated with bounding boxes on the provided imagery. The annotations are provided in the form of closed GeoJSON polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename):\n",
    "    return cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "data_folder = '/media/janko/DATA3/data/datasets/airbus/'\n",
    "imfiles = os.listdir(os.path.join(data_folder, 'images'))\n",
    "imfiles = [os.path.join(data_folder, 'images', f) for f in imfiles if os.path.splitext(f)[-1] == '.jpg']\n",
    "\n",
    "sample = random.choice(imfiles)\n",
    "image = imread(sample)\n",
    "rows, cols, channels = image.shape\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "print('Number of samples:', len(imfiles))\n",
    "print('Image shape:      ', image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6310a9",
   "metadata": {},
   "source": [
    "Check that all images have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(cv2.imread(imfile).shape == (rows, cols, channels) for imfile in tqdm(imfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a0347",
   "metadata": {},
   "source": [
    "Now let's load the file containing the annotations. When you open the csv file, you will see that the geometry information is provided as a string. We need to convert it to a more \"ML friendly\" format. This can be achieved by using a custom converter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75498990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_to_ndarray(x): \n",
    "    return np.array(ast.literal_eval(x))\n",
    "\n",
    "annotations = pd.read_csv('/media/janko/DATA3/data/datasets/airbus/annotations.csv', \n",
    "                          converters={'geometry': geo_to_ndarray})\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a44787",
   "metadata": {},
   "source": [
    "We see a total of 3425 annotated aircarfts. Each aircraft is annotaed with a bounding box in a closed format. Let's now check that there are no objects other than aircrafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(annotations['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86976e81",
   "metadata": {},
   "source": [
    "Also let's make sure that all geometry objects are closed bounding boxes. This means, they contains 5 points and the coordinates of the origin (i.e. the first point) and the last points are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61934eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(len(geo) == 5 for geo in annotations['geometry'])\n",
    "assert all(np.array_equal(geo[0, :], geo[-1, :]) for geo in annotations.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6dc910",
   "metadata": {},
   "source": [
    "It is always important to visualize the data and annotation to make sure that we properly understand and handle the format and that the annotations are, in fact, correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6ae4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = random.choice(imfiles)\n",
    "image = imread(sample)\n",
    "\n",
    "labels = annotations[annotations.image_id == os.path.basename(sample)]\n",
    "points = [geo.reshape((-1, 1, 2)) for geo in labels.geometry]\n",
    "cv2.polylines(image, points, isClosed=True, color=(0, 255, 0), thickness=5)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75988e03",
   "metadata": {},
   "source": [
    "### Data Format Conversion\n",
    "\n",
    "In order to use the YOLOv8 training tools efficiently, we need to use the [YOLOv8 data format](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format).\n",
    "\n",
    "In addition, we will split each into 512x512 crops. This is commonly done to be able to train the model and not incur into memory problems. Therefore, we create a separate dataset with image crops. We will need to adjust the bounding boxes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43604aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_box_coors(box, x_origin, y_origin, width, height, threshold):\n",
    "    \"\"\"Recompute box coordinates to new origin\n",
    "    \n",
    "    Args:\n",
    "        box (np.ndarray): Bounding box coordinates in form (x_min, y_min, x_max, y_max).\n",
    "        x_origin (int): X coordinate of the origin of the new coordinate system.\n",
    "        y_origin (int): Y coordinate of the origin of the new coordinate system.\n",
    "        width (int): Width of the new coordinate system. Recomputed box coordinates\n",
    "            that would fall beyond will be truncated.\n",
    "        threshold (float): Rejection ratio of bounding box after truncation. Recomputed\n",
    "            boxes that are heavily truncated will be discarded.\n",
    "            \n",
    "    Returns:\n",
    "        (tuple): Recomputed bounding boxes in YOLOv8 data format.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Recompute bounds coordinates to new reference\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    x_min, y_min, x_max, y_max = x_min - x_origin, y_min - y_origin, x_max - x_origin, y_max - y_origin\n",
    "\n",
    "    # Return None if the box does not lie within image crop\n",
    "    if (x_min > width) or (x_max < 0.0) or (y_min > height) or (y_max < 0.0):\n",
    "        return None\n",
    "    \n",
    "    # Truncate box x coordinates if necessary\n",
    "    x_max_trunc = min(x_max, width)\n",
    "    x_min_trunc = max(x_min, 0)\n",
    "    # Skip if truncate too much\n",
    "    if (x_max_trunc - x_min_trunc) / (x_max - x_min) < threshold:\n",
    "        return None\n",
    "\n",
    "    # Repeat for y coordinates\n",
    "    y_max_trunc = min(y_max, width) \n",
    "    y_min_trunc = max(y_min, 0) \n",
    "    if (y_max_trunc - y_min_trunc) / (y_max - y_min) < threshold:\n",
    "        return None\n",
    "        \n",
    "    # Convert to YOLOv8 format\n",
    "    x_center = (x_min_trunc + x_max_trunc) / 2.0 / width\n",
    "    y_center = (y_min_trunc + y_max_trunc) / 2.0 / height\n",
    "    x_extend = (x_max_trunc - x_min_trunc) / width\n",
    "    y_extend = (y_max_trunc - y_min_trunc) / height\n",
    "    \n",
    "    return (0, x_center, y_center, x_extend, y_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(geometry):\n",
    "    return np.min(geometry[:, 0]), np.min(geometry[:, 1]), np.max(geometry[:, 0]), np.max(geometry[:, 1])\n",
    "\n",
    "annotations.loc[:,'boxes'] = annotations.loc[:,'geometry'].apply(get_boxes)\n",
    "annotations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63820c",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "Split data into train and validation set and prepare the folders for storing the images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6876e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(annotations['image_id'].unique())\n",
    "np.random.shuffle(fnames)\n",
    "train_split = fnames[0:int(len(fnames)*0.8)]\n",
    "\n",
    "print('Num samples', len(fnames))\n",
    "print('Train split', len(train_split))\n",
    "print('Val split  ', len(fnames) - len(train_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_crops = {'train': '/media/janko/DATA3/data/datasets/airbus/train/images/',\n",
    "                'val': '/media/janko/DATA3/data/datasets/airbus/val/images/'}\n",
    "\n",
    "folder_labels = {'train': '/media/janko/DATA3/data/datasets/airbus/train/labels/',\n",
    "                 'val': '/media/janko/DATA3/data/datasets/airbus/val/labels/'}\n",
    "\n",
    "for folders in [folder_crops, folder_labels]:\n",
    "    for _, folder in folders.items():\n",
    "        if not os.path.isdir(folder):\n",
    "            os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfba4c8",
   "metadata": {},
   "source": [
    "Create image crops and adjust the corresponding labels. The labels will be stored in seperate txt files (one for each image), as required by YOLOv8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 512\n",
    "crop_overlap = 64\n",
    "trunc_th = 0.3\n",
    "step = crop_size - crop_overlap\n",
    "\n",
    "\n",
    "for imfile in (imfiles):    \n",
    "    image = cv2.imread(imfile)\n",
    "    folder = 'train' if os.path.basename(imfile) in train_split else 'val'\n",
    "\n",
    "    # Get annotations for image\n",
    "    labels = annotations[annotations['image_id'] == os.path.basename(imfile)]\n",
    "    img_id = os.path.splitext(os.path.basename(imfile))[0]    \n",
    " \n",
    "    # Extract crops\n",
    "    for x_start in tqdm(np.arange(0, cols - crop_size, step)):\n",
    "        for y_start in np.arange(0, rows - crop_size, step):\n",
    "\n",
    "            x_end = x_start + crop_size\n",
    "            y_end = y_start + crop_size\n",
    "            \n",
    "            filename_crop = os.path.join(folder_crops[folder],\n",
    "                                         img_id + '_' + str(x_start) + '_' + str(y_start) + '.jpg')\n",
    "            filename_label = os.path.join(folder_labels[folder],\n",
    "                                          img_id + '_' + str(x_start) + '_' + str(y_start) + '.txt')\n",
    "                                        \n",
    "            crop = image[y_start:y_end, x_start:x_end, :]\n",
    "            assert crop.shape == (crop_size, crop_size, channels)                \n",
    "            cv2.imwrite(filename_crop, crop)\n",
    "\n",
    "            boxes = [recompute_box_coors(boxes, x_start, y_start, crop_size, crop_size, trunc_th)\n",
    "                     for boxes in labels['boxes']]\n",
    "            boxes = [box for box in boxes if box is not None]            \n",
    "\n",
    "            # save labels\n",
    "            with open(filename_label, 'w+') as f:\n",
    "                for box in boxes:\n",
    "                    f.write(' '.join(str(x) for x in box) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12865d76",
   "metadata": {},
   "source": [
    "Let us visualize the crops and the corresponding labels to check that the cropping has worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d27f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(np.random.choice(os.listdir(folder_crops['train']), 4)):\n",
    "    \n",
    "    # Load image and corresponding labels\n",
    "    image = imread(os.path.join(folder_crops['train'], sample))\n",
    "    with open(os.path.join(folder_labels['train'], sample.replace('.jpg', '.txt')), 'r') as f:\n",
    "        labels = f.readlines()\n",
    "\n",
    "    for box in labels:\n",
    "        box = np.array([d for d in box.split(' ')], dtype=np.float32)\n",
    "        \n",
    "        # Undo coordinate normalization\n",
    "        x_center = box[1] * crop_size\n",
    "        y_center = box[2] * crop_size\n",
    "\n",
    "        width = box[3] * crop_size\n",
    "        height = box[4] * crop_size\n",
    "\n",
    "        # Convert from YOLOv8 format to OpenCV rectangle format\n",
    "        x_start, y_start = int(x_center - width/2), int(y_center - height/2)\n",
    "        x_end, y_end = int(x_center + width/2), int(y_center + height/2)\n",
    "\n",
    "        cv2.rectangle(image, (x_start, y_start), (x_end, y_end), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    plt.subplot(1,4,idx+1), plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf3dfd",
   "metadata": {},
   "source": [
    "### YOLOv8\n",
    "\n",
    "Let's now load the detection model. There are different model [sizes](https://github.com/ultralytics/ultralytics) pretrained on [COCO](https://docs.ultralytics.com/datasets/detect/coco/) that you can chose from. We will use the small model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54443930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8s.pt\")\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c610009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(np.random.choice(imfiles, 4)):\n",
    "    image = cv2.imread(sample)\n",
    "    image = image[1500:, 1500:, :]\n",
    "\n",
    "    result = model.predict(image, conf=0.2)[0]\n",
    "    boxes = result.boxes.cpu().numpy().xyxy.astype(np.int16)\n",
    "\n",
    "    for box_idx, box in enumerate(boxes):\n",
    "        start, stop = box[0:2], box[2:]\n",
    "        cv2.rectangle(image, start, stop, color=(0, 255, 0), thickness=5)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.putText(image, result.names[result.boxes.cls[box_idx].item()], (box[0], box[1]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 6, cv2.LINE_AA)\n",
    "\n",
    "    plt.subplot(1,4,idx+1), plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6bf943",
   "metadata": {},
   "source": [
    "### Train YOLOv8 on Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb17ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "# train and val datasets (image directory or *.txt file with image paths)\n",
    "train: /media/janko/DATA3/data/datasets/airbus/train\n",
    "val: /media/janko/DATA3/data/datasets/airbus/val\n",
    "\n",
    "# number of classes\n",
    "nc: 1\n",
    "\n",
    "# class names\n",
    "names: ['Aircraft']\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d14ccd",
   "metadata": {},
   "source": [
    "Training settings:\n",
    "    https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/janko/data/projects/\"\n",
    "!yolo task=detect mode=train model=yolov8s.pt data={root}/data.yaml epochs=10 imgsz=512 mosaic=0.0 flipud=0.5 scale=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained = YOLO('/home/janko/data/projects/runs/detect/train4/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ae7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in enumerate(np.random.choice(imfiles, 4)):\n",
    "    image = cv2.imread(sample)\n",
    "    image = image[1800:, 1800:, :]\n",
    "\n",
    "    result = model.predict(image, conf=0.2)[0]\n",
    "    boxes = result.boxes.cpu().numpy().xyxy.astype(np.int16)\n",
    "\n",
    "    for box_idx, box in enumerate(boxes):\n",
    "        start, stop = box[0:2], box[2:]\n",
    "        cv2.rectangle(image, start, stop, color=(0, 255, 0), thickness=5)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.putText(image, result.names[result.boxes.cls[box_idx].item()], (box[0], box[1]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 6, cv2.LINE_AA)\n",
    "        \n",
    "    result = retrained.predict(image, conf=0.2)[0]\n",
    "    boxes = result.boxes.cpu().numpy().xyxy.astype(np.int16)\n",
    "\n",
    "    for box_idx, box in enumerate(boxes):\n",
    "        start, stop = box[0:2], box[2:]\n",
    "        cv2.rectangle(image, start, stop, color=(0, 0, 255), thickness=5)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        image = cv2.putText(image, result.names[result.boxes.cls[box_idx].item()], (box[0], box[1]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 6, cv2.LINE_AA)\n",
    "\n",
    "    plt.subplot(1,4,idx+1), plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
